# Event-Driven Big Data Pipeline

🚀 This project demonstrates a real-time log processing pipeline using Kafka, Spark Structured Streaming, Airflow, and AWS S3.

**Key Skills:** Scala/Python, Spark, Kafka, Airflow, Cloud Data Lake

## 📌 Architecture

Kafka → Spark Streaming → S3 → (Optional: Presto/Athena)

*Detailed diagram coming soon!*

## ⚙️ Tech Stack

- Apache Kafka
- Apache Spark Structured Streaming
- Apache Airflow
- AWS S3
- Python / Scala

## ✅ Steps

1. Kafka producer generates logs
2. Spark consumes and processes streams
3. Clean data is stored in S3
4. Airflow orchestrates the pipeline
