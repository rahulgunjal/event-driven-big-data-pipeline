# Event-Driven Big Data Pipeline (Local Setup)

ğŸš€ A fully local project to showcase event-driven data pipelines using Kafka, Spark Structured Streaming, and Airflow.

## ğŸ“Œ Architecture

Kafka (local) â†’ Spark Structured Streaming (local) â†’ Local Folder or MinIO (S3-compatible) â†’ Orchestrated by Airflow (local)

## âš™ï¸ Tech Stack

- Apache Kafka (local)
- Apache Spark Structured Streaming (local)
- Apache Airflow (local)
- Local Storage or MinIO

## âœ… Steps

1. Kafka producer generates logs
2. Spark streaming job consumes and processes logs
3. Processed data saved to local folder or MinIO bucket
4. Airflow orchestrates end-to-end flow

*No cloud costs. 100% local development.*
