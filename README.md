# Event-Driven Big Data Pipeline

ğŸš€ This project demonstrates a real-time log processing pipeline using Kafka, Spark Structured Streaming, Airflow, and AWS S3.

**Key Skills:** Scala/Python, Spark, Kafka, Airflow, Cloud Data Lake

## ğŸ“Œ Architecture

Kafka â†’ Spark Streaming â†’ S3 â†’ (Optional: Presto/Athena)

*Detailed diagram coming soon!*

## âš™ï¸ Tech Stack

- Apache Kafka
- Apache Spark Structured Streaming
- Apache Airflow
- AWS S3
- Python / Scala

## âœ… Steps

1. Kafka producer generates logs
2. Spark consumes and processes streams
3. Clean data is stored in S3
4. Airflow orchestrates the pipeline
